`bottleneck` is a tool that can be used as an initial step for debugging
bottlenecks in your program.

It summarizes runs of your script with the Python profiler and PyTorch's
autograd profiler. Because your script will be profiled, please ensure that it
exits in a finite amount of time.

For more complicated uses of the profilers, please see
https://docs.python.org/3/library/profile.html and
https://pytorch.org/docs/master/autograd.html#profiler for more information.
Running environment analysis...
Running your script with cProfile
Namespace(attn_drop=0.6, dataset='pubmed', early_stop=False, epochs=200, fastmode=False, gpu=0, in_drop=0.6, lr=0.005, negative_slope=0.2, num_heads=8, num_hidden=8, num_layers=1, num_out_heads=1, residual=False, weight_decay=0.0005)
  NumNodes: 19717
  NumEdges: 88651
  NumFeats: 500
  NumClasses: 3
  NumTrainingSamples: 60
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
----Data statistics------'
      #Edges 88651
      #Classes 3
      #Train samples 60
      #Val samples 500
      #Test samples 1000
GAT(
  (gat_layers): ModuleList(
    (0): GATConv(
      (fc): Linear(in_features=500, out_features=64, bias=False)
      (feat_drop): Dropout(p=0.6, inplace=False)
      (attn_drop): Dropout(p=0.6, inplace=False)
      (leaky_relu): LeakyReLU(negative_slope=0.2)
    )
    (1): GATConv(
      (fc): Linear(in_features=64, out_features=3, bias=False)
      (feat_drop): Dropout(p=0.6, inplace=False)
      (attn_drop): Dropout(p=0.6, inplace=False)
      (leaky_relu): LeakyReLU(negative_slope=0.2)
    )
  )
)

Test Accuracy 0.7770
Running your script with the autograd profiler...
Namespace(attn_drop=0.6, dataset='pubmed', early_stop=False, epochs=200, fastmode=False, gpu=0, in_drop=0.6, lr=0.005, negative_slope=0.2, num_heads=8, num_hidden=8, num_layers=1, num_out_heads=1, residual=False, weight_decay=0.0005)
  NumNodes: 19717
  NumEdges: 88651
  NumFeats: 500
  NumClasses: 3
  NumTrainingSamples: 60
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
----Data statistics------'
      #Edges 88651
      #Classes 3
      #Train samples 60
      #Val samples 500
      #Test samples 1000
GAT(
  (gat_layers): ModuleList(
    (0): GATConv(
      (fc): Linear(in_features=500, out_features=64, bias=False)
      (feat_drop): Dropout(p=0.6, inplace=False)
      (attn_drop): Dropout(p=0.6, inplace=False)
      (leaky_relu): LeakyReLU(negative_slope=0.2)
    )
    (1): GATConv(
      (fc): Linear(in_features=64, out_features=3, bias=False)
      (feat_drop): Dropout(p=0.6, inplace=False)
      (attn_drop): Dropout(p=0.6, inplace=False)
      (leaky_relu): LeakyReLU(negative_slope=0.2)
    )
  )
)

Test Accuracy 0.7750
Namespace(attn_drop=0.6, dataset='pubmed', early_stop=False, epochs=200, fastmode=False, gpu=0, in_drop=0.6, lr=0.005, negative_slope=0.2, num_heads=8, num_hidden=8, num_layers=1, num_out_heads=1, residual=False, weight_decay=0.0005)
  NumNodes: 19717
  NumEdges: 88651
  NumFeats: 500
  NumClasses: 3
  NumTrainingSamples: 60
  NumValidationSamples: 500
  NumTestSamples: 1000
Done loading data from cached files.
----Data statistics------'
      #Edges 88651
      #Classes 3
      #Train samples 60
      #Val samples 500
      #Test samples 1000
GAT(
  (gat_layers): ModuleList(
    (0): GATConv(
      (fc): Linear(in_features=500, out_features=64, bias=False)
      (feat_drop): Dropout(p=0.6, inplace=False)
      (attn_drop): Dropout(p=0.6, inplace=False)
      (leaky_relu): LeakyReLU(negative_slope=0.2)
    )
    (1): GATConv(
      (fc): Linear(in_features=64, out_features=3, bias=False)
      (feat_drop): Dropout(p=0.6, inplace=False)
      (attn_drop): Dropout(p=0.6, inplace=False)
      (leaky_relu): LeakyReLU(negative_slope=0.2)
    )
  )
)

Test Accuracy 0.7800
--------------------------------------------------------------------------------
  Environment Summary
--------------------------------------------------------------------------------
PyTorch 1.8.1+cu102 DEBUG compiled w/ CUDA 10.2
Running with Python 3.6 and CUDA 10.2.89

`pip3 list` truncated output:
numpy==1.19.5
torch==1.8.1
--------------------------------------------------------------------------------
  cProfile output
--------------------------------------------------------------------------------
         3351076 function calls (3323793 primitive calls) in 7.807 seconds

   Ordered by: internal time
   List reduced from 2623 to 15 due to restriction <15>

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
       16    3.101    0.194    3.101    0.194 {method 'cuda' of 'torch._C._TensorBase' objects}
      200    0.409    0.002    0.409    0.002 {method 'run_backward' of 'torch._C._EngineBase' objects}
     2406    0.349    0.000    0.497    0.000 /home/ubuntu/anaconda3/envs/dgl-conda/lib/python3.6/site-packages/dgl-0.7-py3.6-linux-x86_64.egg/dgl/sparse.py:77(_gspmm)
      802    0.324    0.000    0.352    0.000 /home/ubuntu/anaconda3/envs/dgl-conda/lib/python3.6/site-packages/dgl-0.7-py3.6-linux-x86_64.egg/dgl/heterograph_index.py:560(in_degrees)
        1    0.323    0.323    0.323    0.323 /home/ubuntu/anaconda3/envs/dgl-conda/lib/python3.6/site-packages/dgl-0.7-py3.6-linux-x86_64.egg/dgl/heterograph_index.py:220(copy_to)
        1    0.284    0.284    0.723    0.723 /home/ubuntu/anaconda3/envs/dgl-conda/lib/python3.6/site-packages/networkx-2.6rc1-py3.6.egg/networkx/classes/digraph.py:630(add_edges_from)
    88651    0.242    0.000    0.401    0.000 /home/ubuntu/anaconda3/envs/dgl-conda/lib/python3.6/site-packages/networkx-2.6rc1-py3.6.egg/networkx/classes/multidigraph.py:398(add_edge)
    88651    0.149    0.000    0.149    0.000 /home/ubuntu/anaconda3/envs/dgl-conda/lib/python3.6/site-packages/networkx-2.6rc1-py3.6.egg/networkx/classes/multigraph.py:374(new_edge_key)
      802    0.144    0.000    1.887    0.002 /home/ubuntu/anaconda3/envs/dgl-conda/lib/python3.6/site-packages/dgl-0.7-py3.6-linux-x86_64.egg/dgl/nn/pytorch/conv/gatconv.py:220(forward)
   325106    0.131    0.000    0.255    0.000 /home/ubuntu/anaconda3/envs/dgl-conda/lib/python3.6/_collections_abc.py:742(__iter__)
    88652    0.096    0.000    0.383    0.000 /home/ubuntu/anaconda3/envs/dgl-conda/lib/python3.6/site-packages/networkx-2.6rc1-py3.6.egg/networkx/convert.py:407(<genexpr>)
     6014    0.094    0.000    0.094    0.000 {built-in method zeros}
        1    0.084    0.084    7.480    7.480 train.py:41(main)
        3    0.063    0.021    0.067    0.022 /home/ubuntu/anaconda3/envs/dgl-conda/lib/python3.6/site-packages/networkx-2.6rc1-py3.6.egg/networkx/classes/digraph.py:426(add_nodes_from)
     2406    0.052    0.000    0.154    0.000 /home/ubuntu/anaconda3/envs/dgl-conda/lib/python3.6/site-packages/dgl-0.7-py3.6-linux-x86_64.egg/dgl/sparse.py:182(_gsddmm)


--------------------------------------------------------------------------------
  autograd profiler output (CPU mode)
--------------------------------------------------------------------------------
        top 15 events sorted by cpu_time_total

--------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                      Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  
--------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                 aten::any        43.40%      50.023ms        43.40%      50.030ms      50.030ms             1  
               EdgeSoftmax         0.28%     318.037us        22.26%      25.662ms      25.662ms             1  
               aten::zeros        21.87%      25.205ms        21.91%      25.252ms      25.252ms             1  
               EdgeSoftmax         0.24%     277.607us        11.15%      12.851ms      12.851ms             1  
             GSpMMBackward         0.05%      56.558us        10.91%      12.576ms      12.576ms             1  
                 aten::exp        10.81%      12.458ms        10.81%      12.460ms      12.460ms             1  
                    GSDDMM         0.07%      80.842us        10.71%      12.349ms      12.349ms             1  
               aten::zeros         0.00%       3.603us        10.64%      12.269ms      12.269ms             1  
               aten::zero_         0.00%       2.896us        10.64%      12.261ms      12.261ms             1  
               aten::fill_        10.63%      12.258ms        10.63%      12.258ms      12.258ms             1  
                  aten::to         0.01%      12.671us         7.24%       8.350ms       8.350ms             1  
               aten::copy_         7.22%       8.319ms         7.22%       8.319ms       8.319ms             1  
             IndexBackward         0.00%       4.813us         5.37%       6.191ms       6.191ms             1  
                 aten::any         5.36%       6.176ms         5.37%       6.191ms       6.191ms             1  
    aten::_index_put_impl_         0.06%      68.377us         5.35%       6.166ms       6.166ms             1  
--------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 115.264ms
Using backend: pytorch
/home/ubuntu/anaconda3/envs/dgl-conda/lib/python3.6/site-packages/dgl-0.7-py3.6-linux-x86_64.egg/dgl/data/utils.py:285: UserWarning: Property dataset.num_labels will be deprecated, please use dataset.num_classes instead.
  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))
/home/ubuntu/anaconda3/envs/dgl-conda/lib/python3.6/site-packages/dgl-0.7-py3.6-linux-x86_64.egg/dgl/data/utils.py:285: UserWarning: Property dataset.graph will be deprecated, please use dataset[0] instead.
  warnings.warn('Property {} will be deprecated, please use {} instead.'.format(old, new))

--------------------------------------------------------------------------------
  autograd profiler output (CUDA mode)
--------------------------------------------------------------------------------
        top 15 events sorted by cpu_time_total

	Because the autograd profiler uses the CUDA event API,
	the CUDA time column reports approximately max(cuda_time, cpu_time).
	Please ignore this output if your code does not use CUDA.

----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                   aten::any        49.13%      50.088ms        49.15%      50.109ms      50.109ms      50.109ms        49.39%      50.109ms      50.109ms             1  
                 EdgeSoftmax         0.32%     326.110us        24.31%      24.784ms      24.784ms     298.500us         0.29%      24.287ms      24.287ms             1  
                   aten::exp         0.04%      44.893us        23.83%      24.297ms      24.297ms      23.889ms        23.54%      23.889ms      23.889ms             1  
                 aten::empty        23.79%      24.252ms        23.79%      24.252ms      24.252ms       0.000us         0.00%       0.000us       0.000us             1  
               GSpMMBackward         0.06%      65.558us        12.28%      12.516ms      12.516ms      63.750us         0.06%      12.516ms      12.516ms             1  
                      GSDDMM         0.09%      89.027us        12.01%      12.250ms      12.250ms      90.500us         0.09%      12.251ms      12.251ms             1  
                 aten::zeros         0.01%       8.389us        11.93%      12.161ms      12.161ms      12.000us         0.01%      12.161ms      12.161ms             1  
                 aten::zero_         0.01%       8.960us        11.92%      12.148ms      12.148ms       8.000us         0.01%      12.149ms      12.149ms             1  
                 aten::fill_        11.91%      12.139ms        11.91%      12.139ms      12.139ms      12.140ms        11.97%      12.140ms      12.140ms             1  
                    aten::to         0.02%      24.527us         8.28%       8.438ms       8.438ms      37.125us         0.04%       8.434ms       8.434ms             1  
                 aten::copy_         8.24%       8.397ms         8.24%       8.397ms       8.397ms       8.397ms         8.28%       8.397ms       8.397ms             1  
                 EdgeSoftmax         0.29%     291.798us         5.50%       5.607ms       5.607ms     298.125us         0.29%       5.607ms       5.607ms             1  
                 aten::zeros         5.05%       5.145ms         5.07%       5.165ms       5.165ms       5.152ms         5.08%       5.168ms       5.168ms             1  
    Optimizer.step#Adam.step         0.53%     540.937us         3.16%       3.223ms       3.223ms     490.750us         0.48%       3.098ms       3.098ms             1  
    Optimizer.step#Adam.step         0.52%     534.552us         3.10%       3.157ms       3.157ms     476.000us         0.47%       3.010ms       3.010ms             1  
----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 101.957ms
Self CUDA time total: 101.462ms

