Namespace(attn_drop=0.6, batch_size=1000, data_cpu=False, dataset='ogbn-products', dropout=0.5, eval_every=5, fan_out='100,200', gpu=0, in_drop=0.6, inductive=False, log_every=20, lr=0.003, negative_slope=0.2, num_epochs=5, num_heads=8, num_hidden=8, num_layers=1, num_out_heads=1, num_workers=4, residual=False, sample_gpu=False)
load ogbn-products
finish loading ogbn-products
finish constructing ogbn-products
Graph(num_nodes=2449029, num_edges=126167053,
      ndata_schemes={'feat': Scheme(shape=(100,), dtype=torch.float32), 'features': Scheme(shape=(100,), dtype=torch.float32), 'labels': Scheme(shape=(), dtype=torch.int64), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}
      edata_schemes={})
GAT(
  (gat_layers): ModuleList(
    (0): GATConv(
      (fc): Linear(in_features=100, out_features=64, bias=False)
      (feat_drop): Dropout(p=0.6, inplace=False)
      (attn_drop): Dropout(p=0.6, inplace=False)
      (leaky_relu): LeakyReLU(negative_slope=0.2)
    )
    (1): GATConv(
      (fc): Linear(in_features=64, out_features=47, bias=False)
      (feat_drop): Dropout(p=0.6, inplace=False)
      (attn_drop): Dropout(p=0.6, inplace=False)
      (leaky_relu): LeakyReLU(negative_slope=0.2)
    )
  )
)
Using backend: pytorch
Epoch 00000 | Step 00000 | Loss 112.5406 | Train Acc 0.0140 | Speed (samples/sec) nan | GPU 4376.9 MB
Epoch 00000 | Step 00020 | Loss 33.5500 | Train Acc 0.0430 | Speed (samples/sec) 2984.3175 | GPU 4420.5 MB
Epoch 00000 | Step 00040 | Loss 6.2004 | Train Acc 0.0600 | Speed (samples/sec) 2897.5141 | GPU 4420.5 MB
Epoch 00000 | Step 00060 | Loss 4.3753 | Train Acc 0.1020 | Speed (samples/sec) 2964.4733 | GPU 4420.5 MB
Epoch 00000 | Step 00080 | Loss 3.5316 | Train Acc 0.1650 | Speed (samples/sec) 2811.5746 | GPU 4428.9 MB
Epoch 00000 | Step 00100 | Loss 3.3754 | Train Acc 0.2200 | Speed (samples/sec) 2818.3201 | GPU 4453.7 MB
Epoch 00000 | Step 00120 | Loss 3.1597 | Train Acc 0.2610 | Speed (samples/sec) 2793.7244 | GPU 4453.7 MB
Epoch 00000 | Step 00140 | Loss 2.8585 | Train Acc 0.3220 | Speed (samples/sec) 2791.3333 | GPU 4453.7 MB
Epoch 00000 | Step 00160 | Loss 2.9367 | Train Acc 0.2680 | Speed (samples/sec) 2806.8423 | GPU 4453.7 MB
Epoch 00000 | Step 00180 | Loss 2.7991 | Train Acc 0.3030 | Speed (samples/sec) 2825.5520 | GPU 4453.7 MB
Epoch Time(s): 114.8317
/home/ubuntu/anaconda3/envs/dgl-conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/ubuntu/anaconda3/envs/dgl-conda/lib/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Epoch 00001 | Step 00000 | Loss 2.7699 | Train Acc 0.3270 | Speed (samples/sec) 2864.1918 | GPU 4453.7 MB
Epoch 00001 | Step 00020 | Loss 2.5885 | Train Acc 0.3530 | Speed (samples/sec) 2880.1416 | GPU 4453.7 MB
Epoch 00001 | Step 00040 | Loss 2.5196 | Train Acc 0.3610 | Speed (samples/sec) 2880.6176 | GPU 4453.7 MB
Epoch 00001 | Step 00060 | Loss 2.4745 | Train Acc 0.3720 | Speed (samples/sec) 2879.6439 | GPU 4453.7 MB
Epoch 00001 | Step 00080 | Loss 2.3684 | Train Acc 0.3640 | Speed (samples/sec) 2856.8699 | GPU 4453.7 MB
Epoch 00001 | Step 00100 | Loss 2.2637 | Train Acc 0.4140 | Speed (samples/sec) 2842.7967 | GPU 4453.7 MB
Epoch 00001 | Step 00120 | Loss 2.2375 | Train Acc 0.4350 | Speed (samples/sec) 2854.7831 | GPU 4453.7 MB
Epoch 00001 | Step 00140 | Loss 2.2485 | Train Acc 0.4190 | Speed (samples/sec) 2853.8214 | GPU 4453.7 MB
Epoch 00001 | Step 00160 | Loss 2.1533 | Train Acc 0.4590 | Speed (samples/sec) 2839.3480 | GPU 4453.7 MB
Epoch 00001 | Step 00180 | Loss 2.1039 | Train Acc 0.4790 | Speed (samples/sec) 2838.4161 | GPU 4453.7 MB
Epoch Time(s): 112.4275
Epoch 00002 | Step 00000 | Loss 2.0419 | Train Acc 0.4710 | Speed (samples/sec) 2868.1558 | GPU 4453.7 MB
Epoch 00002 | Step 00020 | Loss 1.9955 | Train Acc 0.4590 | Speed (samples/sec) 2869.7323 | GPU 4453.7 MB
Epoch 00002 | Step 00040 | Loss 2.0347 | Train Acc 0.5030 | Speed (samples/sec) 2867.0922 | GPU 4453.7 MB
Epoch 00002 | Step 00060 | Loss 1.8992 | Train Acc 0.5200 | Speed (samples/sec) 2866.4954 | GPU 4453.7 MB
Epoch 00002 | Step 00080 | Loss 1.9211 | Train Acc 0.5210 | Speed (samples/sec) 2848.9859 | GPU 4453.7 MB
Epoch 00002 | Step 00100 | Loss 1.8428 | Train Acc 0.5320 | Speed (samples/sec) 2859.9309 | GPU 4453.7 MB
Epoch 00002 | Step 00120 | Loss 1.8114 | Train Acc 0.5620 | Speed (samples/sec) 2855.0396 | GPU 4453.7 MB
Epoch 00002 | Step 00140 | Loss 1.6800 | Train Acc 0.5660 | Speed (samples/sec) 2849.3829 | GPU 4453.7 MB
Epoch 00002 | Step 00160 | Loss 1.7236 | Train Acc 0.5730 | Speed (samples/sec) 2848.1450 | GPU 4453.7 MB
Epoch 00002 | Step 00180 | Loss 1.6459 | Train Acc 0.5850 | Speed (samples/sec) 2842.8257 | GPU 4453.7 MB
Epoch Time(s): 114.4198
Epoch 00003 | Step 00000 | Loss 1.6198 | Train Acc 0.5950 | Speed (samples/sec) 2855.9900 | GPU 4453.7 MB
Epoch 00003 | Step 00020 | Loss 1.6434 | Train Acc 0.5950 | Speed (samples/sec) 2861.1797 | GPU 4453.7 MB
Epoch 00003 | Step 00040 | Loss 1.5285 | Train Acc 0.6510 | Speed (samples/sec) 2867.8856 | GPU 4453.7 MB
Epoch 00003 | Step 00060 | Loss 1.5423 | Train Acc 0.6350 | Speed (samples/sec) 2864.4643 | GPU 4457.8 MB
Epoch 00003 | Step 00080 | Loss 1.5456 | Train Acc 0.6240 | Speed (samples/sec) 2861.1675 | GPU 4457.8 MB
Epoch 00003 | Step 00100 | Loss 1.4328 | Train Acc 0.6560 | Speed (samples/sec) 2860.9324 | GPU 4457.8 MB
Epoch 00003 | Step 00120 | Loss 1.4580 | Train Acc 0.6480 | Speed (samples/sec) 2850.3718 | GPU 4457.8 MB
Epoch 00003 | Step 00140 | Loss 1.3409 | Train Acc 0.6520 | Speed (samples/sec) 2858.5277 | GPU 4457.8 MB
Epoch 00003 | Step 00160 | Loss 1.3914 | Train Acc 0.6850 | Speed (samples/sec) 2853.7134 | GPU 4457.8 MB
Epoch 00003 | Step 00180 | Loss 1.3457 | Train Acc 0.6630 | Speed (samples/sec) 2856.9336 | GPU 4457.8 MB
Epoch Time(s): 111.7978
block: Block(num_src_nodes=1169277, num_dst_nodes=107339, num_edges=8038402) h.size: torch.Size([1169277, 100])
-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                           Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                  rf-leaky-relu         2.69%       2.295ms        22.22%      18.936ms      18.936ms     214.684us         0.27%      18.936ms      18.936ms             1  
                     rf-softmax        20.58%      17.538ms        21.47%      18.298ms      18.298ms      47.453us         0.06%      18.299ms      18.299ms             1  
                    EdgeSoftmax         0.58%     497.699us         0.89%     758.286us     758.286us      13.781ms        17.35%      18.252ms      18.252ms             1  
                       aten::to         0.02%      20.868us        19.44%      16.565ms      16.565ms      26.789us         0.03%      16.563ms      16.563ms             1  
                    aten::copy_        19.40%      16.536ms        19.40%      16.536ms      16.536ms      16.536ms        20.82%      16.536ms      16.536ms             1  
           rf-attn-dot-products        11.72%       9.988ms        11.96%      10.197ms      10.197ms      62.207us         0.08%      10.200ms      10.200ms             1  
                        rf-spmm        10.51%       8.956ms        10.82%       9.224ms       9.224ms     399.773us         0.50%       9.226ms       9.226ms             1  
                          GSpMM         0.26%     219.995us         0.31%     265.560us     265.560us       8.672ms        10.92%       8.826ms       8.826ms             1  
                          rf-FC         8.49%       7.238ms         8.71%       7.426ms       7.426ms     107.042us         0.13%       7.426ms       7.426ms             1  
                  aten::dropout         0.04%      31.375us         0.21%     181.755us      90.877us      25.630us         0.03%       7.413ms       3.707ms             2  
                      aten::sum         0.08%      66.483us         0.09%      77.293us      38.647us       7.397ms         9.31%       7.397ms       3.698ms             2  
           aten::_fused_dropout         0.13%     106.671us         0.18%     150.380us      75.190us       7.388ms         9.30%       7.388ms       3.694ms             2  
                   aten::linear         0.03%      26.432us         0.21%     176.510us     176.510us      65.343us         0.08%       7.319ms       7.319ms             1  
                   aten::matmul         0.02%      20.481us         0.13%     109.490us     109.490us      17.759us         0.02%       7.254ms       7.254ms             1  
                       aten::mm         0.09%      80.571us         0.10%      89.009us      89.009us       7.236ms         9.11%       7.236ms       7.236ms             1  
-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 85.227ms
Self CUDA time total: 79.418ms

Epoch 00004 | Step 00000 | Loss 1.2891 | Train Acc 0.6910 | Speed (samples/sec) 2874.1212 | GPU 4457.8 MB
Epoch 00004 | Step 00020 | Loss 1.3007 | Train Acc 0.6810 | Speed (samples/sec) 2879.3033 | GPU 4457.8 MB
Epoch 00004 | Step 00040 | Loss 1.3124 | Train Acc 0.6910 | Speed (samples/sec) 2877.1665 | GPU 4457.8 MB
Epoch 00004 | Step 00060 | Loss 1.2862 | Train Acc 0.7080 | Speed (samples/sec) 2874.6086 | GPU 4457.8 MB
Epoch 00004 | Step 00080 | Loss 1.2298 | Train Acc 0.7070 | Speed (samples/sec) 2879.5943 | GPU 4508.7 MB
Epoch 00004 | Step 00100 | Loss 1.2273 | Train Acc 0.7140 | Speed (samples/sec) 2875.3420 | GPU 4508.7 MB
Epoch 00004 | Step 00120 | Loss 1.2185 | Train Acc 0.7110 | Speed (samples/sec) 2872.3707 | GPU 4508.7 MB
Epoch 00004 | Step 00140 | Loss 1.2143 | Train Acc 0.7040 | Speed (samples/sec) 2873.2601 | GPU 4508.7 MB
Epoch 00004 | Step 00160 | Loss 1.1568 | Train Acc 0.7170 | Speed (samples/sec) 2872.0009 | GPU 4508.7 MB
Epoch 00004 | Step 00180 | Loss 1.1194 | Train Acc 0.7380 | Speed (samples/sec) 2876.8745 | GPU 4508.7 MB
Epoch Time(s): 112.3230
Traceback (most recent call last):
  File "train_sampling.py", line 215, in <module>
    run(args, device, data)
  File "train_sampling.py", line 137, in run
    print('Avg epoch time: {}'.format(avg / (epoch - 4)))
ZeroDivisionError: division by zero
