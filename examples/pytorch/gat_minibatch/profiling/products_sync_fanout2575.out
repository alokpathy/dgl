Namespace(attn_drop=0.6, batch_size=1000, data_cpu=False, dataset='ogbn-products', dropout=0.5, eval_every=5, fan_out='25,75', gpu=0, in_drop=0.6, inductive=False, log_every=20, lr=0.003, negative_slope=0.2, num_epochs=5, num_heads=8, num_hidden=8, num_layers=1, num_out_heads=1, num_workers=4, residual=False, sample_gpu=False)
load ogbn-products
finish loading ogbn-products
finish constructing ogbn-products
Graph(num_nodes=2449029, num_edges=126167053,
      ndata_schemes={'feat': Scheme(shape=(100,), dtype=torch.float32), 'features': Scheme(shape=(100,), dtype=torch.float32), 'labels': Scheme(shape=(), dtype=torch.int64), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}
      edata_schemes={})
GAT(
  (gat_layers): ModuleList(
    (0): GATConv(
      (fc): Linear(in_features=100, out_features=64, bias=False)
      (feat_drop): Dropout(p=0.6, inplace=False)
      (attn_drop): Dropout(p=0.6, inplace=False)
      (leaky_relu): LeakyReLU(negative_slope=0.2)
    )
    (1): GATConv(
      (fc): Linear(in_features=64, out_features=47, bias=False)
      (feat_drop): Dropout(p=0.6, inplace=False)
      (attn_drop): Dropout(p=0.6, inplace=False)
      (leaky_relu): LeakyReLU(negative_slope=0.2)
    )
  )
)
Using backend: pytorch
Epoch 00000 | Step 00000 | Loss 62.2143 | Train Acc 0.0230 | Speed (samples/sec) nan | GPU 2150.2 MB
Epoch 00000 | Step 00020 | Loss 12.0316 | Train Acc 0.0670 | Speed (samples/sec) 10423.6866 | GPU 2168.9 MB
Epoch 00000 | Step 00040 | Loss 3.9852 | Train Acc 0.0930 | Speed (samples/sec) 10275.3961 | GPU 2168.9 MB
Epoch 00000 | Step 00060 | Loss 3.6012 | Train Acc 0.1600 | Speed (samples/sec) 10270.2435 | GPU 2168.9 MB
Epoch 00000 | Step 00080 | Loss 3.2287 | Train Acc 0.2500 | Speed (samples/sec) 10515.2696 | GPU 2168.9 MB
Epoch 00000 | Step 00100 | Loss 2.9438 | Train Acc 0.2830 | Speed (samples/sec) 10431.6781 | GPU 2168.9 MB
Epoch 00000 | Step 00120 | Loss 2.8595 | Train Acc 0.3060 | Speed (samples/sec) 10476.3662 | GPU 2168.9 MB
Epoch 00000 | Step 00140 | Loss 2.6270 | Train Acc 0.3500 | Speed (samples/sec) 10382.6049 | GPU 2168.9 MB
Epoch 00000 | Step 00160 | Loss 2.5932 | Train Acc 0.3580 | Speed (samples/sec) 10490.5153 | GPU 2168.9 MB
Epoch 00000 | Step 00180 | Loss 2.3074 | Train Acc 0.3930 | Speed (samples/sec) 10498.7068 | GPU 2168.9 MB
Epoch Time(s): 27.5911
/home/ubuntu/anaconda3/envs/dgl-conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/ubuntu/anaconda3/envs/dgl-conda/lib/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Epoch 00001 | Step 00000 | Loss 2.3642 | Train Acc 0.3880 | Speed (samples/sec) 10683.6526 | GPU 2168.9 MB
Epoch 00001 | Step 00020 | Loss 2.2342 | Train Acc 0.4070 | Speed (samples/sec) 10734.2772 | GPU 2173.6 MB
Epoch 00001 | Step 00040 | Loss 2.0664 | Train Acc 0.4530 | Speed (samples/sec) 10730.0115 | GPU 2173.6 MB
Epoch 00001 | Step 00060 | Loss 2.0967 | Train Acc 0.4350 | Speed (samples/sec) 10619.8988 | GPU 2173.6 MB
Epoch 00001 | Step 00080 | Loss 1.9749 | Train Acc 0.5020 | Speed (samples/sec) 10600.3353 | GPU 2173.6 MB
Epoch 00001 | Step 00100 | Loss 1.9281 | Train Acc 0.4940 | Speed (samples/sec) 10522.5924 | GPU 2173.6 MB
Epoch 00001 | Step 00120 | Loss 1.8567 | Train Acc 0.5300 | Speed (samples/sec) 10510.3164 | GPU 2173.6 MB
Epoch 00001 | Step 00140 | Loss 1.7788 | Train Acc 0.5490 | Speed (samples/sec) 10393.7395 | GPU 2173.6 MB
Epoch 00001 | Step 00160 | Loss 1.7603 | Train Acc 0.5630 | Speed (samples/sec) 10467.7985 | GPU 2173.6 MB
Epoch 00001 | Step 00180 | Loss 1.6480 | Train Acc 0.5720 | Speed (samples/sec) 10451.1026 | GPU 2173.6 MB
Epoch Time(s): 27.4867
Epoch 00002 | Step 00000 | Loss 1.5657 | Train Acc 0.6210 | Speed (samples/sec) 10532.1908 | GPU 2173.6 MB
Epoch 00002 | Step 00020 | Loss 1.5948 | Train Acc 0.6070 | Speed (samples/sec) 10538.2751 | GPU 2173.6 MB
Epoch 00002 | Step 00040 | Loss 1.5905 | Train Acc 0.5970 | Speed (samples/sec) 10490.7734 | GPU 2173.6 MB
Epoch 00002 | Step 00060 | Loss 1.4945 | Train Acc 0.6400 | Speed (samples/sec) 10499.8369 | GPU 2173.6 MB
Epoch 00002 | Step 00080 | Loss 1.4219 | Train Acc 0.6490 | Speed (samples/sec) 10506.0072 | GPU 2173.6 MB
Epoch 00002 | Step 00100 | Loss 1.4985 | Train Acc 0.6460 | Speed (samples/sec) 10497.7995 | GPU 2173.6 MB
Epoch 00002 | Step 00120 | Loss 1.4195 | Train Acc 0.6450 | Speed (samples/sec) 10496.2522 | GPU 2173.6 MB
Epoch 00002 | Step 00140 | Loss 1.4032 | Train Acc 0.6530 | Speed (samples/sec) 10496.2021 | GPU 2173.6 MB
Epoch 00002 | Step 00160 | Loss 1.2741 | Train Acc 0.6970 | Speed (samples/sec) 10526.8278 | GPU 2173.6 MB
Epoch 00002 | Step 00180 | Loss 1.2599 | Train Acc 0.6990 | Speed (samples/sec) 10515.9591 | GPU 2173.6 MB
Epoch Time(s): 26.4071
Epoch 00003 | Step 00000 | Loss 1.3569 | Train Acc 0.6850 | Speed (samples/sec) 10587.3941 | GPU 2173.6 MB
Epoch 00003 | Step 00020 | Loss 1.2989 | Train Acc 0.6820 | Speed (samples/sec) 10605.1808 | GPU 2173.6 MB
Epoch 00003 | Step 00040 | Loss 1.2340 | Train Acc 0.6980 | Speed (samples/sec) 10576.4203 | GPU 2173.6 MB
Epoch 00003 | Step 00060 | Loss 1.1814 | Train Acc 0.7130 | Speed (samples/sec) 10562.5198 | GPU 2173.6 MB
Epoch 00003 | Step 00080 | Loss 1.2993 | Train Acc 0.6770 | Speed (samples/sec) 10601.2914 | GPU 2173.6 MB
Epoch 00003 | Step 00100 | Loss 1.1585 | Train Acc 0.7250 | Speed (samples/sec) 10593.4942 | GPU 2173.6 MB
Epoch 00003 | Step 00120 | Loss 1.1425 | Train Acc 0.7230 | Speed (samples/sec) 10612.0185 | GPU 2173.6 MB
Epoch 00003 | Step 00140 | Loss 1.1612 | Train Acc 0.7230 | Speed (samples/sec) 10584.3273 | GPU 2173.6 MB
Epoch 00003 | Step 00160 | Loss 1.1600 | Train Acc 0.7260 | Speed (samples/sec) 10574.1542 | GPU 2173.6 MB
Epoch 00003 | Step 00180 | Loss 1.1373 | Train Acc 0.7270 | Speed (samples/sec) 10590.3640 | GPU 2173.6 MB
Epoch Time(s): 26.1806
block: Block(num_src_nodes=581614, num_dst_nodes=62071, num_edges=1505008) h.size: torch.Size([581614, 100])
-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                           Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                  rf-leaky-relu         2.68%     545.995us        18.37%       3.742ms       3.742ms     161.121us         0.82%       3.742ms       3.742ms             1  
                       aten::to         0.10%      19.376us        15.38%       3.134ms       3.134ms      25.088us         0.13%       3.132ms       3.132ms             1  
                    aten::copy_        15.25%       3.108ms        15.25%       3.108ms       3.108ms       3.107ms        15.85%       3.107ms       3.107ms             1  
           rf-attn-dot-products        13.92%       2.837ms        15.03%       3.062ms       3.062ms      47.871us         0.24%       3.063ms       3.063ms             1  
                  aten::dropout         0.14%      27.630us         0.84%     171.231us      85.616us      21.502us         0.11%       3.001ms       1.500ms             2  
           aten::_fused_dropout         0.50%     101.287us         0.70%     143.601us      71.801us       2.979ms        15.20%       2.979ms       1.490ms             2  
                     rf-softmax        10.41%       2.120ms        13.32%       2.713ms       2.713ms      35.616us         0.18%       2.715ms       2.715ms             1  
                    EdgeSoftmax         1.89%     384.617us         2.91%     592.319us     592.319us       1.760ms         8.98%       2.679ms       2.679ms             1  
                   rf-feat-drop        12.14%       2.473ms        12.64%       2.574ms       2.574ms     121.216us         0.62%       2.575ms       2.575ms             1  
                          rf-FC        11.76%       2.396ms        12.58%       2.563ms       2.563ms      85.889us         0.44%       2.564ms       2.564ms             1  
                   aten::linear         0.12%      23.738us         0.77%     157.471us     157.471us      61.472us         0.31%       2.478ms       2.478ms             1  
                   aten::matmul         0.09%      17.525us         0.46%      93.992us      93.992us      14.400us         0.07%       2.417ms       2.417ms             1  
                       aten::mm         0.35%      71.755us         0.38%      76.467us      76.467us       2.402ms        12.25%       2.402ms       2.402ms             1  
                        rf-spmm         8.56%       1.743ms         9.55%       1.945ms       1.945ms     326.594us         1.67%       1.947ms       1.947ms             1  
                          GSpMM         0.76%     154.984us         0.98%     199.748us     199.748us       1.518ms         7.74%       1.620ms       1.620ms             1  
-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 20.372ms
Self CUDA time total: 19.606ms

Epoch 00004 | Step 00000 | Loss 1.0924 | Train Acc 0.7300 | Speed (samples/sec) 10620.0707 | GPU 2173.6 MB
Epoch 00004 | Step 00020 | Loss 1.1233 | Train Acc 0.7270 | Speed (samples/sec) 10617.8715 | GPU 2173.6 MB
Epoch 00004 | Step 00040 | Loss 1.1482 | Train Acc 0.7050 | Speed (samples/sec) 10622.8054 | GPU 2173.6 MB
Epoch 00004 | Step 00060 | Loss 1.1318 | Train Acc 0.7370 | Speed (samples/sec) 10640.1131 | GPU 2173.6 MB
Epoch 00004 | Step 00080 | Loss 1.0532 | Train Acc 0.7190 | Speed (samples/sec) 10633.3516 | GPU 2173.6 MB
Epoch 00004 | Step 00100 | Loss 1.0601 | Train Acc 0.7350 | Speed (samples/sec) 10621.2219 | GPU 2173.6 MB
Epoch 00004 | Step 00120 | Loss 1.0565 | Train Acc 0.7490 | Speed (samples/sec) 10606.2522 | GPU 2173.6 MB
Epoch 00004 | Step 00140 | Loss 1.0114 | Train Acc 0.7370 | Speed (samples/sec) 10615.9906 | GPU 2173.6 MB
Epoch 00004 | Step 00160 | Loss 1.0004 | Train Acc 0.7620 | Speed (samples/sec) 10599.2715 | GPU 2173.6 MB
Epoch 00004 | Step 00180 | Loss 0.9546 | Train Acc 0.7480 | Speed (samples/sec) 10583.5954 | GPU 2173.6 MB
Epoch Time(s): 26.3755
Traceback (most recent call last):
  File "train_sampling.py", line 215, in <module>
    run(args, device, data)
  File "train_sampling.py", line 137, in run
    print('Avg epoch time: {}'.format(avg / (epoch - 4)))
ZeroDivisionError: division by zero
