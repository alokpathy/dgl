Namespace(attn_drop=0.6, batch_size=1000, data_cpu=False, dataset='reddit', dropout=0.5, eval_every=5, fan_out='100,200', gpu=0, in_drop=0.6, inductive=False, log_every=20, lr=0.003, negative_slope=0.2, num_epochs=5, num_heads=8, num_hidden=8, num_layers=1, num_out_heads=1, num_workers=4, residual=False, sample_gpu=False)
Graph(num_nodes=232965, num_edges=114848857,
      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'feat': Scheme(shape=(602,), dtype=torch.float32), 'test_mask': Scheme(shape=(), dtype=torch.bool), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'features': Scheme(shape=(602,), dtype=torch.float32), 'labels': Scheme(shape=(), dtype=torch.int64)}
      edata_schemes={})
GAT(
  (gat_layers): ModuleList(
    (0): GATConv(
      (fc): Linear(in_features=602, out_features=64, bias=False)
      (feat_drop): Dropout(p=0.6, inplace=False)
      (attn_drop): Dropout(p=0.6, inplace=False)
      (leaky_relu): LeakyReLU(negative_slope=0.2)
    )
    (1): GATConv(
      (fc): Linear(in_features=64, out_features=41, bias=False)
      (feat_drop): Dropout(p=0.6, inplace=False)
      (attn_drop): Dropout(p=0.6, inplace=False)
      (leaky_relu): LeakyReLU(negative_slope=0.2)
    )
  )
)
Using backend: pytorch
Epoch 00000 | Step 00000 | Loss 51.9572 | Train Acc 0.0440 | Speed (samples/sec) nan | GPU 3283.0 MB
Epoch 00000 | Step 00020 | Loss 11.9161 | Train Acc 0.0800 | Speed (samples/sec) 3071.8355 | GPU 3324.6 MB
Epoch 00000 | Step 00040 | Loss 4.2267 | Train Acc 0.0790 | Speed (samples/sec) 2926.2111 | GPU 3326.5 MB
Epoch 00000 | Step 00060 | Loss 3.6474 | Train Acc 0.1570 | Speed (samples/sec) 2977.8330 | GPU 3326.5 MB
Epoch 00000 | Step 00080 | Loss 3.3228 | Train Acc 0.2630 | Speed (samples/sec) 2900.6814 | GPU 3326.5 MB
Epoch 00000 | Step 00100 | Loss 3.0469 | Train Acc 0.3480 | Speed (samples/sec) 2902.0435 | GPU 3326.5 MB
Epoch 00000 | Step 00120 | Loss 2.6593 | Train Acc 0.4160 | Speed (samples/sec) 2908.8109 | GPU 3328.1 MB
Epoch 00000 | Step 00140 | Loss 2.4654 | Train Acc 0.4180 | Speed (samples/sec) 2864.6292 | GPU 3332.7 MB
Epoch Time(s): 93.4500
/home/ubuntu/anaconda3/envs/dgl-conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/ubuntu/anaconda3/envs/dgl-conda/lib/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Epoch 00001 | Step 00000 | Loss 2.2810 | Train Acc 0.5080 | Speed (samples/sec) 2908.0615 | GPU 3332.7 MB
Epoch 00001 | Step 00020 | Loss 2.2247 | Train Acc 0.5230 | Speed (samples/sec) 2933.5624 | GPU 3333.7 MB
Epoch 00001 | Step 00040 | Loss 2.0583 | Train Acc 0.5370 | Speed (samples/sec) 2941.5417 | GPU 3333.7 MB
Epoch 00001 | Step 00060 | Loss 1.8455 | Train Acc 0.6080 | Speed (samples/sec) 2922.6697 | GPU 3333.7 MB
Epoch 00001 | Step 00080 | Loss 1.6881 | Train Acc 0.6250 | Speed (samples/sec) 2932.3248 | GPU 3333.7 MB
Epoch 00001 | Step 00100 | Loss 1.6260 | Train Acc 0.6600 | Speed (samples/sec) 2916.0186 | GPU 3333.7 MB
Epoch 00001 | Step 00120 | Loss 1.5702 | Train Acc 0.6770 | Speed (samples/sec) 2901.8606 | GPU 3333.7 MB
Epoch 00001 | Step 00140 | Loss 1.6155 | Train Acc 0.6760 | Speed (samples/sec) 2903.0229 | GPU 3333.7 MB
Epoch Time(s): 92.9802
Epoch 00002 | Step 00000 | Loss 1.4149 | Train Acc 0.6970 | Speed (samples/sec) 2920.2360 | GPU 3333.7 MB
Epoch 00002 | Step 00020 | Loss 1.3940 | Train Acc 0.7280 | Speed (samples/sec) 2943.3613 | GPU 3334.3 MB
Epoch 00002 | Step 00040 | Loss 1.2220 | Train Acc 0.7560 | Speed (samples/sec) 2952.2507 | GPU 3334.3 MB
Epoch 00002 | Step 00060 | Loss 1.1783 | Train Acc 0.7590 | Speed (samples/sec) 2960.2535 | GPU 3334.3 MB
Epoch 00002 | Step 00080 | Loss 1.1690 | Train Acc 0.7750 | Speed (samples/sec) 2957.7212 | GPU 3334.3 MB
Epoch 00002 | Step 00100 | Loss 1.0474 | Train Acc 0.7690 | Speed (samples/sec) 2957.7414 | GPU 3334.3 MB
Epoch 00002 | Step 00120 | Loss 0.9829 | Train Acc 0.8070 | Speed (samples/sec) 2949.2894 | GPU 3334.3 MB
Epoch 00002 | Step 00140 | Loss 1.0327 | Train Acc 0.7810 | Speed (samples/sec) 2931.4888 | GPU 3337.8 MB
Epoch Time(s): 90.6352
Epoch 00003 | Step 00000 | Loss 1.0492 | Train Acc 0.7830 | Speed (samples/sec) 2953.3186 | GPU 3337.8 MB
Epoch 00003 | Step 00020 | Loss 1.0351 | Train Acc 0.7970 | Speed (samples/sec) 2958.6384 | GPU 3337.8 MB
Epoch 00003 | Step 00040 | Loss 0.8907 | Train Acc 0.8180 | Speed (samples/sec) 2961.2797 | GPU 3337.8 MB
Epoch 00003 | Step 00060 | Loss 0.9071 | Train Acc 0.8320 | Speed (samples/sec) 2956.8303 | GPU 3337.8 MB
Epoch 00003 | Step 00080 | Loss 0.8471 | Train Acc 0.8370 | Speed (samples/sec) 2959.8538 | GPU 3337.8 MB
Epoch 00003 | Step 00100 | Loss 0.8856 | Train Acc 0.8290 | Speed (samples/sec) 2944.5223 | GPU 3337.8 MB
Epoch 00003 | Step 00120 | Loss 0.8170 | Train Acc 0.8380 | Speed (samples/sec) 2939.9772 | GPU 3337.8 MB
Epoch 00003 | Step 00140 | Loss 0.9121 | Train Acc 0.8510 | Speed (samples/sec) 2945.1279 | GPU 3337.8 MB
Epoch Time(s): 91.0158
block: Block(num_src_nodes=217911, num_dst_nodes=83236, num_edges=8069195) h.size: torch.Size([217911, 602])
-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                           Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                  rf-leaky-relu         3.20%       2.279ms        25.93%      18.481ms      18.481ms     185.855us         0.26%      18.482ms      18.482ms             1  
                     rf-softmax        24.74%      17.632ms        25.64%      18.274ms      18.274ms      39.359us         0.06%      18.275ms      18.275ms             1  
                    EdgeSoftmax         0.58%     414.577us         0.90%     640.872us     640.872us      13.780ms        19.34%      18.236ms      18.236ms             1  
                       aten::to         0.03%      22.964us        22.64%      16.138ms      16.138ms      30.402us         0.04%      16.137ms      16.137ms             1  
                    aten::copy_        22.60%      16.107ms        22.60%      16.107ms      16.107ms      16.106ms        22.60%      16.106ms      16.106ms             1  
                        rf-spmm        11.88%       8.464ms        12.23%       8.715ms       8.715ms     316.438us         0.44%       8.717ms       8.717ms             1  
                          GSpMM         0.29%     205.313us         0.35%     249.910us     249.910us       8.270ms        11.61%       8.400ms       8.400ms             1  
                  aten::dropout         0.04%      26.577us         0.24%     172.953us      86.476us      20.105us         0.03%       8.228ms       4.114ms             2  
           aten::_fused_dropout         0.15%     105.758us         0.21%     146.376us      73.188us       8.208ms        11.52%       8.208ms       4.104ms             2  
                          rf-FC         9.54%       6.797ms         9.79%       6.975ms       6.975ms     103.328us         0.15%       6.976ms       6.976ms             1  
                   aten::linear         0.04%      29.605us         0.23%     166.763us     166.763us      64.961us         0.09%       6.872ms       6.872ms             1  
                   aten::matmul         0.03%      19.572us         0.14%     100.426us     100.426us      16.607us         0.02%       6.807ms       6.807ms             1  
                       aten::mm         0.11%      75.558us         0.11%      80.854us      80.854us       6.791ms         9.53%       6.791ms       6.791ms             1  
                       rf-sddmm         8.69%       6.192ms         9.08%       6.472ms       6.472ms     510.592us         0.72%       6.473ms       6.473ms             1  
                         GSDDMM         0.29%     205.947us         0.39%     278.927us     278.927us       4.798ms         6.73%       5.963ms       5.963ms             1  
-------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 71.271ms
Self CUDA time total: 71.259ms

Epoch 00004 | Step 00000 | Loss 0.8553 | Train Acc 0.8320 | Speed (samples/sec) 2955.2832 | GPU 3337.8 MB
Epoch 00004 | Step 00020 | Loss 0.7015 | Train Acc 0.8590 | Speed (samples/sec) 2959.3708 | GPU 3337.8 MB
Epoch 00004 | Step 00040 | Loss 0.7213 | Train Acc 0.8650 | Speed (samples/sec) 2959.8783 | GPU 3337.8 MB
Epoch 00004 | Step 00060 | Loss 0.7584 | Train Acc 0.8490 | Speed (samples/sec) 2960.4790 | GPU 3337.8 MB
Epoch 00004 | Step 00080 | Loss 0.6869 | Train Acc 0.8670 | Speed (samples/sec) 2956.4145 | GPU 3337.8 MB
Epoch 00004 | Step 00100 | Loss 0.7745 | Train Acc 0.8490 | Speed (samples/sec) 2957.6679 | GPU 3337.8 MB
Epoch 00004 | Step 00120 | Loss 0.7445 | Train Acc 0.8790 | Speed (samples/sec) 2952.9221 | GPU 3337.8 MB
Epoch 00004 | Step 00140 | Loss 0.7612 | Train Acc 0.8630 | Speed (samples/sec) 2946.4192 | GPU 3346.7 MB
Epoch Time(s): 90.7977
Traceback (most recent call last):
  File "train_sampling.py", line 215, in <module>
    run(args, device, data)
  File "train_sampling.py", line 137, in run
    print('Avg epoch time: {}'.format(avg / (epoch - 4)))
ZeroDivisionError: division by zero
